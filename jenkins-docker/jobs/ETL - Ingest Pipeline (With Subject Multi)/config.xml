<?xml version='1.1' encoding='UTF-8'?>
<flow-definition plugin="workflow-job@1400.v7fd111b_ec82f">
  <actions>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobAction plugin="pipeline-model-definition@2.2218.v56d0cda_37c72"/>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction plugin="pipeline-model-definition@2.2218.v56d0cda_37c72">
      <jobProperties/>
      <triggers/>
      <parameters/>
      <options/>
    </org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction>
  </actions>
  <description>This pipeline should be run when ingesting any study with a subject multi file&#xd;
&#xd;
Prerequisite:&#xd;
BDC Managed Inputs should be updated.&#xd;
&#xd;
</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <org.jenkinsci.plugins.workflow.job.properties.DisableConcurrentBuildsJobProperty>
      <abortPrevious>false</abortPrevious>
    </org.jenkinsci.plugins.workflow.job.properties.DisableConcurrentBuildsJobProperty>
    <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@332.va_1ee476d8f6d">
      <autoRebuild>false</autoRebuild>
      <rebuildDisabled>false</rebuildDisabled>
    </com.sonyericsson.rebuild.RebuildSettings>
  </properties>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsFlowDefinition" plugin="workflow-cps@4000.v5198556e9cea_">
    <script>import groovy.json.JsonSlurper;
pipeline {
    agent any 
    stages {
        stage(&apos;Import data(fully compliant studies)&apos;) {
            steps {
                withAWS(role:&apos;dbgap-etl&apos;, roleAccount:&apos;736265540791&apos;, duration: 3600, roleSessionName: &apos;subject-multi-pipeline&apos;) {
                    s3Download(file:&apos;Managed_Inputs.csv&apos;, bucket:&apos;avillach-73-bdcatalyst-etl&apos;, path:&apos;general/resources/Managed_Inputs.csv&apos;, force:true)
                }
                
                script {
                    def inputs = readCSV file: &apos;Managed_Inputs.csv&apos;, format: CSVFormat.DEFAULT.withHeader()
                    def importRuns = [:]
                    def inputCount = 0
                    inputs.each() {
                        if  (
                            it[&apos;Data is ready to process&apos;] == &apos;Yes&apos; &amp; it[&quot;Data Processed&quot;] == &apos;No&apos; &amp; it[&quot;Has Multi&quot;] == &apos;Yes&apos;
                            &amp; (it[&apos;Study Type&apos;].equalsIgnoreCase(&apos;topmed&apos;) || it[&apos;Study Type&apos;].equalsIgnoreCase(&apos;parent&apos;) || it[&apos;Study Type&apos;].equalsIgnoreCase(&apos;substudy&apos;)) 
                        ) {
                             def stdy_id = it[&quot;Study Identifier&quot;]
                             importRuns[&quot;importRuns${stdy_id}&quot;] = {
                                build job: &apos;ETL - Pull Data from NHLBI S3&apos;,
                                    parameters: [
                                        string(name: &apos;study_abv_name&apos;, value: it[&apos;Study Abbreviated Name&apos;]),
                                        string(name: &apos;nhlbi_account_number&apos;, value: it[&apos;Pheno Ingest NHLBI Account&apos;]),
                                        string(name: &apos;bucket_s3_names&apos;, value: it[&apos;Pheno Ingest buckets&apos;])
                                    ]
                             }
                        }
                    }
                    parallel importRuns
                }
            }
        }
        stage(&apos;Refreshing Curated Data Dictionaries&apos;) {
            steps {
                script {
                    build job: &apos;ETL - Import Metadata From git&apos;
                }
            }
        }
        
        stage(&apos;Decode Data&apos;) {
            steps {
                withAWS(role:&apos;dbgap-etl&apos;, roleAccount:&apos;736265540791&apos;, duration: 3600, roleSessionName: &apos;subject-multi-pipeline&apos;) {
                    s3Download(file:&apos;Managed_Inputs.csv&apos;, bucket:&apos;avillach-73-bdcatalyst-etl&apos;, path:&apos;general/resources/Managed_Inputs.csv&apos;, force:true)
                }
                
                script {
                    def inputs = readCSV file: &apos;Managed_Inputs.csv&apos;, format: CSVFormat.DEFAULT.withHeader()
                    def decodeRuns = [:]
                    def inputCount = 0
                    inputs.each() {
                        if  (it[&apos;Data is ready to process&apos;] == &apos;Yes&apos; &amp; it[&quot;Data Processed&quot;] == &apos;No&apos; &amp; it[&quot;Has Multi&quot;] == &apos;Yes&apos;) {
                             def stdy_id = it[&quot;Study Identifier&quot;]
                             decodeRuns[&quot;decodeRuns${stdy_id}&quot;] = {
                                build job: &apos;ETL - DBGap Decode Data&apos;,
                                    parameters: [
                                        string(name: &apos;abv_name&apos;, value: it[&apos;Study Abbreviated Name&apos;]),
                                        string(name: &apos;stdy_id&apos;, value: it[&apos;Study Identifier&apos;])
                                    ]
                             }
                        }
                    }
                    parallel decodeRuns
                }
            }
        }
        stage(&apos;Sequence Patients&apos;) {
            steps {
                withAWS(role:&apos;dbgap-etl&apos;, roleAccount:&apos;736265540791&apos;, duration: 3600, roleSessionName: &apos;semi-compliant-pipeline&apos;) {
                    s3Download(file:&apos;Managed_Inputs.csv&apos;, bucket:&apos;avillach-73-bdcatalyst-etl&apos;, path:&apos;general/resources/Managed_Inputs.csv&apos;, force:true)
                }
                
                script {
                    def inputs = readCSV file: &apos;Managed_Inputs.csv&apos;, format: CSVFormat.DEFAULT.withHeader()
                    def sequenceRuns = [:]
                    def inputCount = 0
                    inputs.each() {
                        inputCount += 1;
                        if  (it.get(&apos;Data is ready to process&apos;) == &apos;Yes&apos; &amp; it.get(&apos;Data Processed&apos;) == &apos;No&apos; &amp; it.get(&apos;Has Multi&apos;) == &apos;Yes&apos;) {
                            
                             sequenceRuns[&quot;sequenceRuns${inputCount}&quot;] = {
                                build job: &apos;ETL - DBGap Sequence Patients&apos;, 
                                    parameters: [
                                        string(name: &apos;abv_name&apos;, value: it.get(&quot;Study Abbreviated Name&quot;))
                                    ]
                             }
                        }
                    }
                    parallel sequenceRuns
                }
            }
        }
        stage(&apos;Generate Study Metadata&apos;) {
            steps {
                script {
                    build job: &apos;ETL - Generate Metadata&apos;
                }     
            } 
        }
        stage(&apos;Generate Global Variables&apos;) { 
            steps {
                script {
                    build job: &apos;ETL - Generate DBGAP Global Variables&apos;
                }
                script {
                    build job: &apos;ETL - Generate DBGAP Global All Concepts&apos;
                }
            }
        }
        stage(&apos;Create Mapping Files for Studies Not Processed Yet&apos;) { 
            steps {
                
                script {
                    def mappingRuns = [:]
                    mappingRuns[&quot;noncomp-mapping&quot;] = {
                        build job: &apos;ETL - Generate Non_DBGAP Mapping Files&apos;
                    }
                    mappingRuns[&quot;comp-mapping&quot;] = {
                       build job: &apos;ETL - Generate DBGAP Mapping Files&apos;
                    }
                    parallel mappingRuns
                }
            }
        }
        
        stage(&apos;Generate All Concepts for Studies Not Processed Yet&apos;) { 
            steps {
                script {
                    def allConceptsRuns = [:]
                    allConceptsRuns[&quot;noncomp-allconcepts&quot;] = {
                        build job: &apos;ETL - Generate Non-DBGap All Concepts with Data Type Analyzer&apos;
                    }
                    allConceptsRuns[&quot;comp-allconcepts&quot;] = {
                       build job: &apos;ETL - Generate DBGAP All Concepts with Data Type Analyzer&apos;
                    }
                    parallel allConceptsRuns
                }
            }
        }
        

        stage(&apos;Generating Genomic Index Files Where Needed&apos;) {
            steps {
                script {
                    build job: &apos;ETL - Generate Genomic Index Files For Newly Ingested Studies&apos;
                }
            }
        }
    }
}</script>
    <sandbox>true</sandbox>
  </definition>
  <triggers/>
  <disabled>false</disabled>
</flow-definition>