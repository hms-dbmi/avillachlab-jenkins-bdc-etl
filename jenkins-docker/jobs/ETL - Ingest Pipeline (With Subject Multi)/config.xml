<?xml version='1.1' encoding='UTF-8'?>
<flow-definition plugin="workflow-job@1400.v7fd111b_ec82f">
  <actions>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobAction plugin="pipeline-model-definition@2.2205.vc9522a_9d5711"/>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction plugin="pipeline-model-definition@2.2205.vc9522a_9d5711">
      <jobProperties/>
      <triggers/>
      <parameters/>
      <options/>
    </org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction>
  </actions>
  <description>This pipeline should be run when ingesting any study with a subject multi file&#xd;
&#xd;
Prerequisite:&#xd;
BDC Managed Inputs should be updated.&#xd;
&#xd;
</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <org.jenkinsci.plugins.workflow.job.properties.DisableConcurrentBuildsJobProperty>
      <abortPrevious>false</abortPrevious>
    </org.jenkinsci.plugins.workflow.job.properties.DisableConcurrentBuildsJobProperty>
    <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@332.va_1ee476d8f6d">
      <autoRebuild>false</autoRebuild>
      <rebuildDisabled>false</rebuildDisabled>
    </com.sonyericsson.rebuild.RebuildSettings>
  </properties>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsFlowDefinition" plugin="workflow-cps@3943.v3519a_3260660">
    <script>import groovy.json.JsonSlurper;
pipeline {
    agent any 
    stages {
        stage(&apos;Decode Data&apos;) {
            steps {
                withAWS(role:&apos;dbgap-etl&apos;, roleAccount:&apos;736265540791&apos;, duration: 3600, roleSessionName: &apos;subject-multi-pipeline&apos;) {
                    s3Download(file:&apos;Managed_Inputs.csv&apos;, bucket:&apos;avillach-73-bdcatalyst-etl&apos;, path:&apos;general/resources/Managed_Inputs.csv&apos;, force:true)
                }
                
                script {
                    def inputs = readCSV file: &apos;Managed_Inputs.csv&apos;
                    def decodeRuns = [:]
                    def inputCount = 0
                    inputs.each() {
                        inputCount+=1
                        if  (it[7] == &apos;Yes&apos; &amp; it[8] == &apos;No&apos; &amp; it[14] == &apos;Yes&apos;) {
                             def stdy_id = it[1]
                             decodeRuns[&quot;decodeRuns${stdy_id}&quot;] = {
                                build job: &apos;ETL - DBGap Decode Data&apos;,
                                    parameters: [
                                        string(name: &apos;abv_name&apos;, value: it[0]), 
                                        string(name: &apos;stdy_id&apos;, value: it[1])
                                    ]
                             }
                        }
                    }
                    parallel decodeRuns
                }
            }
        }
        stage(&apos;Sequence Patients&apos;) {
            steps {
                withAWS(role:&apos;dbgap-etl&apos;, roleAccount:&apos;736265540791&apos;, duration: 3600, roleSessionName: &apos;semi-compliant-pipeline&apos;) {
                    s3Download(file:&apos;Managed_Inputs.csv&apos;, bucket:&apos;avillach-73-bdcatalyst-etl&apos;, path:&apos;general/resources/Managed_Inputs.csv&apos;, force:true)
                }
                
                script {
                    def inputs = readCSV file: &apos;Managed_Inputs.csv&apos;
                    def sequenceRuns = [:]
                    def inputCount = 0
                    inputs.each() {
                        inputCount+=1
                        if  (it[7] == &apos;Yes&apos; &amp; it[8] == &apos;No&apos; &amp; it[14] == &apos;Yes&apos;) {
                            echo &apos;Sequencing patients for &apos; + it[1]
                             sequenceRuns[&quot;sequenceRuns${inputCount}&quot;] = {
                                build job: &apos;ETL - DBGap Sequence Patients&apos;, 
                                    parameters: [
                                        string(name: &apos;abv_name&apos;, value: it[0])
                                    ]
                             }
                        }
                    }
                    parallel sequenceRuns
                }
            }
        }
        stage(&apos;Generate Study Metadata&apos;) {
            steps {
                script {
                    build job: &apos;ETL - Generate Metadata&apos;
                }     
            } 
        }
        stage(&apos;Generate Global Variables&apos;) { 
            steps {
                script {
                    build job: &apos;ETL - Generate DBGAP Global Variables&apos;
                }
                script {
                    build job: &apos;ETL - Generate DBGAP Global All Concepts&apos;
                }
            }
        }
        stage(&apos;Create Mapping Files for Studies Not Processed Yet&apos;) { 
            steps {
                
                script {
                    def mappingRuns = [:]
                    mappingRuns[&quot;noncomp-mapping&quot;] = {
                        build job: &apos;ETL - Generate Non_DBGAP Mapping Files&apos;
                    }
                    mappingRuns[&quot;comp-mapping&quot;] = {
                       build job: &apos;ETL - Generate DBGAP Mapping Files&apos;
                    }
                    parallel mappingRuns
                }
            }
        }
        
        stage(&apos;Generate All Concepts for Studies Not Processed Yet&apos;) { 
            steps {
                script {
                    def allConceptsRuns = [:]
                    allConceptsRuns[&quot;noncomp-allconcepts&quot;] = {
                        build job: &apos;ETL - Generate Non-DBGap All Concepts with Data Type Analyzer&apos;
                    }
                    allConceptsRuns[&quot;comp-allconcepts&quot;] = {
                       build job: &apos;ETL - Generate DBGAP All Concepts with Data Type Analyzer&apos;
                    }
                    parallel allConceptsRuns
                }
            }
        }
        
        stage(&apos;Refreshing Curated Data Dictionaries&apos;) {
            steps {
                script {
                    build job: &apos;ETL - Import Metadata From git&apos;
                }
            }
        }
        stage(&apos;Generating Genomic Index Files Where Needed&apos;) {
            steps {
                script {
                    build job: &apos;ETL - Generate Genomic Index Files For Newly Ingested Studies&apos;
                }
            }
        }
    }
}</script>
    <sandbox>true</sandbox>
  </definition>
  <triggers/>
  <disabled>false</disabled>
</flow-definition>