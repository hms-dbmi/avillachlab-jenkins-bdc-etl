<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description>This job will generate all files needed to build the global allconcepts for all studies with a subject multi</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@332.va_1ee476d8f6d">
      <autoRebuild>false</autoRebuild>
      <rebuildDisabled>false</rebuildDisabled>
    </com.sonyericsson.rebuild.RebuildSettings>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>managed_inputs</name>
          <defaultValue>s3://avillach-73-bdcatalyst-etl/general/resources/Managed_Inputs.csv</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.plugins.git.GitSCM" plugin="git@5.5.2">
    <configVersion>2</configVersion>
    <userRemoteConfigs>
      <hudson.plugins.git.UserRemoteConfig>
        <url>https://github.com/hms-dbmi/ETL-MissionControl-dbgap-submodule</url>
      </hudson.plugins.git.UserRemoteConfig>
    </userRemoteConfigs>
    <branches>
      <hudson.plugins.git.BranchSpec>
        <name>*/master</name>
      </hudson.plugins.git.BranchSpec>
    </branches>
    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
    <submoduleCfg class="empty-list"/>
    <extensions/>
  </scm>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <jdk>(System)</jdk>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
# Managed Inputs

aws sts assume-role --duration-seconds 3600 --role-arn arn:aws:iam::736265540791:role/dbgap-etl --role-session-name &quot;s3-test&quot; &gt; assume-role-output.txt
export AWS_ACCESS_KEY_ID=`grep AccessKeyId assume-role-output.txt | cut -d &apos;:&apos; -f 2 | sed &quot;s/[ ,\&quot;]//g&quot;`
export AWS_SECRET_ACCESS_KEY=`grep SecretAccessKey assume-role-output.txt | cut -d &apos;:&apos; -f 2 | sed &quot;s/[ ,\&quot;]//g&quot;`
export AWS_SESSION_TOKEN=`grep SessionToken assume-role-output.txt | cut -d &apos;:&apos; -f 2 | sed &quot;s/[ ,\&quot;]//g&quot;`

mkdir data
mkdir completed
mkdir completed_study_consents
mkdir hierarchies
mkdir processing

aws s3 cp ${managed_inputs} data/

aws s3 cp s3://avillach-73-bdcatalyst-etl/general/data/metadata.json data/

csvcut -c &quot;Study Abbreviated Name&quot;,&quot;Study Identifier&quot;,&quot;Study Type&quot;,&quot;Data is ready to process&quot;,&quot;Data Processed&quot;,&quot;Has Multi&quot; data/Managed_Inputs.csv | csvformat -E &gt; inputs.csv

rm -rf uniq_ids.txt

# Generate mappings
cat	inputs.csv
IFS=&apos;,&apos;
[ ! -f inputs.csv ]
while read abv_name stdy_id stdy_type data_ready data_processed has_multi
do
	
  	if [[ &quot;${data_ready,,}&quot; == &quot;yes&quot; ]]; then

	   if [[ ${has_multi,,} == &quot;yes&quot; ]]; then
        echo &quot;pulling data for $abv_name&quot;
         
        aws s3 cp s3://avillach-73-bdcatalyst-etl/${abv_name,,}/rawData/ data/ --recursive --exclude &quot;*&quot; --include &quot;${stdy_id}*subject.multi*&quot; --include &quot;${stdy_id}*Subject.Multi*&quot; --include &quot;${stdy_id}*Subject.MULTI*&quot; --include &quot;${stdy_id}*sample.multi*&quot; --include &quot;${stdy_id}*Sample.Multi*&quot; --include &quot;${stdy_id}*Sample.MULTI*&quot; --quiet
        
		aws s3 cp s3://avillach-73-bdcatalyst-etl/${abv_name,,}/decoded_data/ data/decoded/ --recursive --exclude &quot;*&quot; --include &quot;${stdy_id}*subject.multi*&quot; --include &quot;${stdy_id}*Subject.Multi*&quot; --include &quot;${stdy_id}*Subject.MULTI*&quot; --include &quot;${stdy_id}*sample.multi*&quot; --include &quot;${stdy_id}*Sample.Multi*&quot; --include &quot;${stdy_id}*Sample.MULTI*&quot; --quiet
        
		aws s3 cp s3://avillach-73-bdcatalyst-etl/${abv_name,,}/data/${abv_name^^}_PatientMapping.v2.csv data/ --quiet
        
        #clean out old study consents so malformed/outdated consents dont get re-added
        aws s3 rm s3://avillach-73-bdcatalyst-etl/general/globalvars/study_consents/ --recursive --exclude &quot;*&quot; --include &quot;${abv_name}_${stdy_id}*&quot;
      fi
   else 
      echo &quot;$abv_name not ready for processing or does not have multi files&quot;
   fi
done &lt; inputs.csv

echo &quot;pulling harmonized data set&quot;
aws s3 cp s3://avillach-73-bdcatalyst-etl/hrmn/completed/HRMN_allConcepts.csv data/ --quiet

aws s3 cp s3://avillach-73-bdcatalyst-etl/hrmn/data/HRMN_PatientMapping.v2.csv data/ --quiet

unset AWS_ACCESS_KEY_ID
unset AWS_SECRET_ACCESS_KEY
unset AWS_SESSION_TOKEN

echo &quot;generating consent group data&quot;
java -jar jars/ConsentGroupGenerator.jar
echo &quot;generating phs id data&quot;
java -jar jars/PHSIdGenerator.jar
echo &quot;generating rootnode data&quot;
java -jar jars/RootNodeGenerator.jar
echo &quot;generating genomic sample id global var data&quot;
java -jar jars/GenomicSampIdGlobalVarGenerator.jar
echo &quot;generating studies consents&quot;
java -jar jars/StudiesConsentsGenerator.jar -writedir ./completed_study_consents/


aws sts assume-role --duration-seconds 3600 --role-arn arn:aws:iam::736265540791:role/dbgap-etl --role-session-name &quot;s3-test&quot; &gt; assume-role-output.txt

export AWS_ACCESS_KEY_ID=`grep AccessKeyId assume-role-output.txt | cut -d &apos;:&apos; -f 2 | sed &quot;s/[ ,\&quot;]//g&quot;`
export AWS_SECRET_ACCESS_KEY=`grep SecretAccessKey assume-role-output.txt | cut -d &apos;:&apos; -f 2 | sed &quot;s/[ ,\&quot;]//g&quot;`
export AWS_SESSION_TOKEN=`grep SessionToken assume-role-output.txt | cut -d &apos;:&apos; -f 2 | sed &quot;s/[ ,\&quot;]//g&quot;`

aws s3 cp completed/ s3://avillach-73-bdcatalyst-etl/general/data/ --recursive

aws s3 cp s3://avillach-73-bdcatalyst-etl/general/mappings/mapping.prerootnodes.csv mappings/mapping.csv
# Append the mappings for genomic sample id global variable.
cat completed/SampleId_mapping.csv &gt;&gt; mappings/mapping.csv
# Append the mappings for root node global variable.
cat completed/rootnode_mapping.csv &gt;&gt; mappings/mapping.csv

aws s3 cp mappings/mapping.csv s3://avillach-73-bdcatalyst-etl/general/mappings/mapping.postrootnodes.csv
aws s3 cp completed_study_consents/ s3://avillach-73-bdcatalyst-etl/general/globalvars/study_consents/ --recursive
</command>
      <configuredLocalRules/>
      <unstableReturn>255</unstableReturn>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers>
    <hudson.plugins.ws__cleanup.PreBuildCleanup plugin="ws-cleanup@0.47">
      <deleteDirs>false</deleteDirs>
      <cleanupParameter></cleanupParameter>
      <externalDelete></externalDelete>
      <disableDeferredWipeout>false</disableDeferredWipeout>
    </hudson.plugins.ws__cleanup.PreBuildCleanup>
  </buildWrappers>
</project>