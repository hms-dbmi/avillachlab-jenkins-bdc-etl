<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description>Using this to migrate to new jenkins building methodology.</description>
  <keepDependencies>false</keepDependencies>
  <properties/>
  <scm class="hudson.plugins.git.GitSCM" plugin="git@5.2.1">
    <configVersion>2</configVersion>
    <userRemoteConfigs>
      <hudson.plugins.git.UserRemoteConfig>
        <url>https://github.com/hms-dbmi/avillachlab-jenkins-bdc-etl</url>
      </hudson.plugins.git.UserRemoteConfig>
    </userRemoteConfigs>
    <branches>
      <hudson.plugins.git.BranchSpec>
        <name>*/some-architecture-work</name>
      </hudson.plugins.git.BranchSpec>
    </branches>
    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
    <submoduleCfg class="empty-list"/>
    <extensions/>
  </scm>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
set -e

cd &quot;${WORKSPACE}/jenkins-docker-remodeled&quot;

# This code block below is ported in from in-boundary work.
# The bash_functions mostly contains functions that handle AWS cli mechanisms ( creating snapshots, swapping stacks, assuming roles, etc... ).
# However we may want to use this same mechanism for ETL AWS cli functions

# Source folder containing the scripts
#source_scripts_folder=&quot;${JENKINS_HOME}/workspace/Bash_Functions/&quot;
#ls -la &quot;$source_scripts_folder&quot;
# Iterate through the files in the folder and source them
#for script_file in &quot;$source_scripts_folder&quot;*.sh; do
#    chmod +x &quot;$script_file&quot;
#    if [ -f &quot;$script_file&quot; ] &amp;&amp; [ -x &quot;$script_file&quot; ]; then
# echo &quot;sourcing $script_file&quot;
#        source &quot;$script_file&quot;
#    fi
#done
git_commit_short=`echo ${GIT_COMMIT} |head -c7`

# Create certs directory
mkdir -p certs/

# Download Jenkins config file from s3
echo &quot;Download Jenkins config file from s3&quot;
aws --region us-east-1 s3 cp &quot;${jenkins_config_s3_location}&quot; ./config.xml

# Build Container
echo &quot;# Build Container&quot;
docker build \
  --build-arg TERRAFORM_DISTRO=&quot;${CI_terraform_distro}&quot; \
  --build-arg CONFIG_XML_FILE=&quot;./config.xml&quot; \
  --build-arg PLUGINS_FILE=&quot;./plugins.txt&quot; \
  --build-arg SCRIPT_APPROVAL_FILE=&quot;./scriptApproval.xml&quot; \
  --build-arg JOBS_DIR=&quot;./jobs/&quot; \
  --build-arg PKCS12_FILE=&quot;&quot; \
  --build-arg PKCS12_PASS=&quot;&quot; \
  --build-arg HUDSON_TASKS_FILE=&quot;./hudson.tasks.Maven.xml&quot; \
  -t jenkins:$git_commit_short .

# Time to put it in a private repo instead of just a image tar on s3.
docker save jenkins:$git_commit_short | gzip &gt; jenkins.tar.gz

aws s3 cp jenkins.tar.gz s3://${stack_s3_bucket}/containers/jenkins/ 

cd &quot;${WORKSPACE}&quot;</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers>
    <hudson.plugins.ws__cleanup.PreBuildCleanup plugin="ws-cleanup@0.45">
      <deleteDirs>false</deleteDirs>
      <cleanupParameter></cleanupParameter>
      <externalDelete></externalDelete>
      <disableDeferredWipeout>false</disableDeferredWipeout>
    </hudson.plugins.ws__cleanup.PreBuildCleanup>
  </buildWrappers>
</project>